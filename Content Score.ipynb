{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6916b417-545c-45ca-ad8f-939759d2d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 Content Score\n",
    "# Build content representations for movies: a Story (semantic) channel and a Creator (metadata) channel.\n",
    "# Produces artifacts:\n",
    "# - story_embeddings.npy (n_movies x D)  or story_tfidf.npz + vectorizer (fallback)\n",
    "# - creators_tfidf.npz + vectorizer\n",
    "# - movies_with_content_meta.csv (contains content-related columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992006fb-7890-4259-a04a-97c8accb34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45714f83-06b8-493a-820d-b8492b51c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "IN_CSV = \"movies_with_numeric_score.csv\"    # output of numeric score notebook\n",
    "OUT_META = \"movies_with_content_meta.csv\"\n",
    "STORY_EMB_NPY = \"story_embeddings.npy\"      # sentence-embeddings\n",
    "STORY_TFIDF_VEC = \"story_tfidf_vectorizer.joblib\"\n",
    "STORY_TFIDF_NPZ = \"story_tfidf.npz\"\n",
    "CREATORS_VECTORIZER = \"creators_tfidf.joblib\"\n",
    "CREATORS_MATRIX = \"creators_tfidf.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e875836b-46f7-420c-a72b-4309fb00602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TOP_CAST = 5            # keep only top-k cast members\n",
    "DIRECTOR_BOOST = 8      # repetition multiplier for director tokens\n",
    "CAST_BOOST = 3          # repetition multiplier for cast tokens\n",
    "WRITERS_BOOST = 3\n",
    "PRODUCERS_BOOST = 2\n",
    "\n",
    "# TF-IDF params for creators channel\n",
    "CREATORS_TFIDF_PARAMS = {\n",
    "    'max_features': 15000,\n",
    "    'ngram_range': (1,1),\n",
    "    'stop_words': 'english',\n",
    "    'dtype': np.float32,\n",
    "    'norm': 'l2',\n",
    "    'min_df': 3\n",
    "}\n",
    "\n",
    "# TF-IDF params for fallback story channel (if SBERT unavailable)\n",
    "STORY_TFIDF_PARAMS = {\n",
    "    'max_features': 20000,\n",
    "    'ngram_range': (1,2),\n",
    "    'stop_words': 'english',\n",
    "    'dtype': np.float32,\n",
    "    'norm': 'l2',\n",
    "    'min_df': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b2e0bc-6c09-43a1-9607-7cf7aed864e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helpers for tokenization (same style used earlier) ----------\n",
    "SEP_PATTERN = r'[,\\|;/]+'\n",
    "WHITESPACE_RE = re.compile(r'\\s+')\n",
    "UNKNOWN_TOKEN = 'unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43893ac0-8c70-47b9-9f19-78a21da4527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_token(s):\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r'\\(.*?\\)', '', s)\n",
    "    s = re.sub(r'[^0-9a-zA-Z]+', ' ', s)\n",
    "    s = WHITESPACE_RE.sub(' ', s).strip()\n",
    "    s = s.replace(' ', '_')\n",
    "    if not s or s == 'unknown':\n",
    "        return None\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb457ef-7aab-4a2e-a91a-2a459c645ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_prefix(field_name, cell_value, max_items=None):\n",
    "    if pd.isna(cell_value) or str(cell_value).strip() == '':\n",
    "        return []\n",
    "    parts = re.split(SEP_PATTERN, str(cell_value))\n",
    "    if max_items is not None:\n",
    "        parts = parts[:max_items]\n",
    "    tokens = []\n",
    "    for p in parts:\n",
    "        norm = normalize_token(p)\n",
    "        if norm is None:\n",
    "            continue\n",
    "        tokens.append(f\"{field_name}_{norm}\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecdbb0a6-1987-40f4-947a-dc851e92c304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded movies_with_numeric_score.csv with 29937 rows\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load dataframe ----------\n",
    "if not os.path.exists(IN_CSV):\n",
    "    raise FileNotFoundError(f\"Input not found: {IN_CSV}. Run numeric score notebook first.\")\n",
    "\n",
    "df = pd.read_csv(IN_CSV)\n",
    "print(f\"Loaded {IN_CSV} with {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f57caff-bf88-4c39-b2ef-cb19b46a98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure important columns exist\n",
    "for col in ['overview','tagline','genres','cast','director','writers','producers','production_companies','spoken_languages']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1bf697d-d29f-49e2-9af4-6cb12eda4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building story_text (overview + tagline)...\n",
      "SBERT not available or failed — will fallback to TF-IDF for story channel. No module named 'sentence_transformers'\n",
      "Fitting TF-IDF on story_text (fallback)...\n",
      "Saved story TF-IDF -> story_tfidf_vectorizer.joblib, story_tfidf.npz\n",
      "Building creators content tokens (director, cast, writers, producers, genres, companies)...\n"
     ]
    }
   ],
   "source": [
    "# ---------- Build story_text column (overview + tagline) ----------\n",
    "print(\"Building story_text (overview + tagline)...\")\n",
    "df['story_text'] = df['overview'].fillna('') + ' ' + df['tagline'].fillna('')\n",
    "# small cleanup\n",
    "df['story_text'] = df['story_text'].astype(str).str.replace('\\n',' ').str.strip()\n",
    "\n",
    "# ---------- Try Sentence-BERT for story embeddings (recommended) ----------\n",
    "use_sbert = True\n",
    "story_embeddings = None\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"SentenceTransformer available — building story embeddings (SBERT)...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    story_embeddings = model.encode(df['story_text'].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "    np.save(STORY_EMB_NPY, story_embeddings)\n",
    "    print(f\"Saved story embeddings to {STORY_EMB_NPY} (shape={story_embeddings.shape})\")\n",
    "except Exception as e:\n",
    "    use_sbert = False\n",
    "    print(\"SBERT not available or failed — will fallback to TF-IDF for story channel.\", e)\n",
    "\n",
    "if not use_sbert:\n",
    "    # Fit TF-IDF on story text and save\n",
    "    print(\"Fitting TF-IDF on story_text (fallback)...\")\n",
    "    story_tfidf = TfidfVectorizer(**STORY_TFIDF_PARAMS)\n",
    "    story_matrix = story_tfidf.fit_transform(df['story_text'].astype(str).values)\n",
    "    joblib.dump(story_tfidf, STORY_TFIDF_VEC)\n",
    "    sparse.save_npz(STORY_TFIDF_NPZ, story_matrix)\n",
    "    print(f\"Saved story TF-IDF -> {STORY_TFIDF_VEC}, {STORY_TFIDF_NPZ}\")\n",
    "\n",
    "# ---------- Build creators/content tokens (weighted) ----------\n",
    "print(\"Building creators content tokens (director, cast, writers, producers, genres, companies)...\")\n",
    "creator_texts = []\n",
    "for row in df.itertuples(index=False):\n",
    "    tokens = []\n",
    "    # director (keep 1, strong boost)\n",
    "    tokens += split_and_prefix('director', getattr(row, 'director', ''), max_items=1) * DIRECTOR_BOOST\n",
    "    # director_of_photography\n",
    "    tokens += split_and_prefix('director_of_photography', getattr(row, 'director_of_photography', ''), max_items=1) * 3\n",
    "    # cast (top-k)\n",
    "    tokens += split_and_prefix('cast', getattr(row, 'cast', ''), max_items=TOP_CAST) * CAST_BOOST\n",
    "    # writers and producers\n",
    "    tokens += split_and_prefix('writers', getattr(row, 'writers', ''), max_items=3) * WRITERS_BOOST\n",
    "    tokens += split_and_prefix('producers', getattr(row, 'producers', ''), max_items=3) * PRODUCERS_BOOST\n",
    "    # genres and companies and countries and languages\n",
    "    tokens += split_and_prefix('genres', getattr(row, 'genres', ''))\n",
    "    tokens += split_and_prefix('production_companies', getattr(row, 'production_companies', ''))\n",
    "    tokens += split_and_prefix('production_countries', getattr(row, 'production_countries', ''))\n",
    "    tokens += split_and_prefix('spoken_languages', getattr(row, 'spoken_languages', ''))\n",
    "    creator_texts.append(' '.join(tokens))\n",
    "\n",
    "# attach to df\n",
    "df['creators_content'] = creator_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1050bb51-e3f1-47e8-81b4-d155e5df26a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF on creators_content...\n",
      "Saved creators tfidf -> creators_tfidf.joblib, creators_tfidf.npz; shape=(29937, 15000)\n"
     ]
    }
   ],
   "source": [
    "# Fit TF-IDF on creators_content\n",
    "print(\"Fitting TF-IDF on creators_content...\")\n",
    "creators_tfidf = TfidfVectorizer(**CREATORS_TFIDF_PARAMS)\n",
    "creators_matrix = creators_tfidf.fit_transform(df['creators_content'].values)\n",
    "joblib.dump(creators_tfidf, CREATORS_VECTORIZER)\n",
    "sparse.save_npz(CREATORS_MATRIX, creators_matrix)\n",
    "print(f\"Saved creators tfidf -> {CREATORS_VECTORIZER}, {CREATORS_MATRIX}; shape={creators_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee69ab3-35d8-4176-bb59-b7d6b5d3a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata with content columns to movies_with_content_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Save augmented metadata ----------\n",
    "df_out = df.copy()\n",
    "df_out.to_csv(OUT_META, index=False)\n",
    "print(f\"Saved metadata with content columns to {OUT_META}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df77cd95-0f54-4cb3-99a6-710fcc177dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Helper functions (for use in final notebook) ----------\n",
    "\n",
    "def compute_story_sim_to_user(selected_indices=None, selected_genres=None):\n",
    "    \"\"\"Return story similarity vector (0-1) for all movies given user-selected indices.\n",
    "       Uses SBERT if available else story TF-IDF.\n",
    "    \"\"\"\n",
    "    if selected_indices is None or len(selected_indices)==0:\n",
    "        raise ValueError(\"Provide list of selected movie indices\")\n",
    "\n",
    "    if use_sbert and story_embeddings is not None:\n",
    "        # average embeddings\n",
    "        user_vec = np.mean(story_embeddings[selected_indices], axis=0, keepdims=True)\n",
    "        sims = (story_embeddings @ user_vec.T).flatten()  # cosine-like if embeddings normalized\n",
    "        # if embeddings not normalized, compute cosine\n",
    "        # normalize\n",
    "        sims = (sims - sims.min()) / (sims.max() - sims.min() + 1e-12)\n",
    "        return sims\n",
    "    else:\n",
    "        # load story_tfidf and matrix if not in memory\n",
    "        if 'story_matrix' not in globals():\n",
    "            story_tfidf = joblib.load(STORY_TFIDF_VEC)\n",
    "            story_matrix = sparse.load_npz(STORY_TFIDF_NPZ)\n",
    "        else:\n",
    "            story_tfidf = globals().get('story_tfidf')\n",
    "            story_matrix = globals().get('story_matrix')\n",
    "        # average rows\n",
    "        sel = story_matrix[selected_indices]\n",
    "        user_vec = sel.mean(axis=0)\n",
    "        if not sparse.issparse(user_vec):\n",
    "            user_vec = sparse.csr_matrix(user_vec)\n",
    "        sims = story_matrix.dot(user_vec.T).A.flatten()\n",
    "        sims = (sims - sims.min()) / (sims.max() - sims.min() + 1e-12)\n",
    "        return sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b69908-0c86-4441-a8a5-f4a01c3d3fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content score artifacts ready. Use compute_story_sim_to_user() and compute_creators_sim_to_user() in the final notebook to produce hybrid content similarity.\n"
     ]
    }
   ],
   "source": [
    "def compute_creators_sim_to_user(selected_indices=None, selected_genres=None):\n",
    "    if selected_indices is None or len(selected_indices)==0:\n",
    "        raise ValueError(\"Provide list of selected movie indices\")\n",
    "    # average creators_matrix rows\n",
    "    if 'creators_matrix' not in globals():\n",
    "        creators_tfidf = joblib.load(CREATORS_VECTORIZER)\n",
    "        creators_matrix = sparse.load_npz(CREATORS_MATRIX)\n",
    "    else:\n",
    "        creators_matrix = globals().get('creators_matrix')\n",
    "    sel = creators_matrix[selected_indices]\n",
    "    user_vec = sel.mean(axis=0)\n",
    "    if not sparse.issparse(user_vec):\n",
    "        user_vec = sparse.csr_matrix(user_vec)\n",
    "    sims = creators_matrix.dot(user_vec.T).A.flatten()\n",
    "    sims = (sims - sims.min()) / (sims.max() - sims.min() + 1e-12)\n",
    "    return sims\n",
    "\n",
    "print('\\nContent score artifacts ready. Use compute_story_sim_to_user() and compute_creators_sim_to_user() in the final notebook to produce hybrid content similarity.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044011dc-ec46-47b1-8def-9d45cb40cf37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84778d6-ac6b-4d56-bb97-dfcd4e3935e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
